import json
from dataclasses import dataclass
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base
from dotenv import load_dotenv
import openai
import os
from typing import List
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import pandas as pd
import csv
from collections import defaultdict
import subprocess
import base64
import cv2
from typing import Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os
from pathlib import Path
from langchain_core.messages import HumanMessage, AIMessage
from fastmcp import Context

"""
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='debug.log',  # ë¡œê·¸ë¥¼ ê¸°ë¡í•  íŒŒì¼ ì´ë¦„
    filemode='a',  # 'w'ëŠ” ë®ì–´ì“°ê¸°, 'a'ëŠ” ì´ì–´ì“°ê¸°
    encoding='utf-8'
)
"""

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

model = ChatOpenAI(
    model="gpt-5",
    use_responses_api=True,
    output_version="responses/v1",
    extra_body={
        "text": {"verbosity": "low"},
        "reasoning": {"effort": "minimal"},
    },
    api_key=api_key 
)

mcp = FastMCP("VerifyMCP", instructions="Observe the current state and determine if the previous task was completed successfully.")

def capture_adb_screen_image(filename: str = "screen.png") -> Path:
    try:
        subprocess.run("adb shell screencap -p /sdcard/screen.png", shell=True, check=True)
        subprocess.run(f"adb pull /sdcard/screen.png {filename}", shell=True, check=True)
        return Path(filename)
    except Exception as e:
        print(f"[ERROR] {e}")
        return None

def encode_image_to_base64(image_path: Path) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def query_screen_with_llm(image_path: Path, question: str) -> str:
    if not image_path or not image_path.exists():
        raise RuntimeError("Image file does not exist.")

    descriptive_prompt = (
        "You are a precise UI analyst. You must follow this rule to determine the state of radio buttons:\n"
        "First, describe everything you see in the current screen in detail. "
        "It is a fact that one option is always selected. Based on this rule, first describe the state of each radio button, and then answer the original question:\n"
        "When checking the IP connection, determine it as connected if the connection icon is green, not the Wi-Fi icon.\n\n"
    )
    
    # ë‘ í”„ë¡¬í”„íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    final_question = descriptive_prompt + question

    base64_image = encode_image_to_base64(image_path)
    image_url = f"data:image/png;base64,{base64_image}"

    msg = HumanMessage(content=[
        {"type": "text", "text": final_question},
        {"type": "image_url", "image_url": {"url": image_url}},
    ])

    llm = model
    res = llm.invoke([msg])

    # ì‘ë‹µ ì²˜ë¦¬
    if isinstance(res.content, list):
        for item in res.content:
            if isinstance(item, dict) and item.get('type') == 'text':
                return item.get('text', '').strip()
    elif isinstance(res.content, str):
        return res.content.strip()

    return "No valid text content found in the response."

def get_log():
    file_path = os.path.join('resource', 'log_info.txt')
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        return f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except IOError as e:
        return f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

@mcp.tool()
async def adb_screen_vlm(ctx: Context, question: str) -> dict:
    """
    FastMCP Tool: ADB í™”ë©´ ìº¡ì²˜ í›„ LLM(VLM) ì§ˆì˜
    Args:
        question (str): í™”ë©´ ì´ë¯¸ì§€ì— ëŒ€í•´ LLMì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸
    Returns:
        dict: {"success": bool, "answer": str}
    """
    #logging.info(f"ğŸ•µï¸â€â™‚ï¸ 'adb_screen_vlm' tool called with question: {question}")
    #question += "ì–´ë–¤ ìš”ì†Œê°€ ì„ íƒë˜ì—ˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²½ìš°, ë°”ë¡œ ì˜†ì˜ ë¼ë””ì˜¤ ë²„íŠ¼ì´ í™œì„±í™” ë˜ì–´ìˆë‹¤ë©´ ì„ íƒë˜ì—ˆë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    screen_file = capture_adb_screen_image()
    if not screen_file:
        return {"success": False, "answer": "Failed to capture screen."}

    try:
        answer = query_screen_with_llm(screen_file, question)
        return {"success": True, "answer": answer}
    except Exception as e:
        return {"success": False, "answer": str(e)}
    
@mcp.tool()
def analyze_log(step: str, expected_result: str) -> dict:
    logs = get_log()

    prompt = f"""
    ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ìˆ˜í–‰í•œ ì‘ì—… ë‹¨ê³„(step), ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼(expected result), 
    ê·¸ë¦¬ê³  ì‹¤ì œ ìˆ˜ì§‘ëœ ë¡œê·¸(logs)ì…ë‹ˆë‹¤.

    [Step ì„¤ëª…]
    {step}

    [ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼]
    {expected_result}

    [Log ëª©ë¡]
    {logs}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì‘ì—…ì´ ì„±ê³µí–ˆëŠ”ì§€, ì‹¤íŒ¨í–ˆëŠ”ì§€, ì•„ë‹ˆë©´ ë¶ˆí™•ì‹¤í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

    ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”. 
    ì•„ë˜ì™€ ê°™ì€ ë‘ ê°œì˜ í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
    - "result": "success" | "fail" | "uncertain"
    - "reason": ê°„ë‹¨í•œ ì´ìœ  ì„¤ëª…
    """

    response = model.invoke([HumanMessage(content=prompt)])

    # response.content ì²˜ë¦¬
    raw_output = response.content
    if isinstance(raw_output, list):
        raw_output = "".join(item.get("text", "") for item in raw_output if isinstance(item, dict))
    raw_output = raw_output.strip()

    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
    import re, json
    match = re.search(r"\{.*\}", raw_output, re.DOTALL)
    if match:
        raw_output = match.group(0)

    try:
        parsed = json.loads(raw_output)
        parsed["result"] = parsed.get("result", "").lower()
        return parsed
    except Exception:
        return {
            "result": "uncertain",
            "reason": f"Failed to parse model output as JSON. Raw output: {raw_output}"
        }
    
@mcp.prompt()
def verify_prompt(step: str, expected_result: str) -> list[base.Message]:
    system_content = f"""   
ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” ì‹ ì¤‘í•˜ê³  ì² ì €í•œ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

[ì—­í• ]
- stepì€ ì´ë¯¸ ì‹¤í–‰ëœ ìƒíƒœì…ë‹ˆë‹¤.
- expected_resultëŠ” stepì´ ì„±ê³µí–ˆì„ ë•Œ ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.
- ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì •ë³´ì™€ íˆ´ì„ í™œìš©í•˜ì—¬ expected_resultê°€ ì¶©ì¡±ë˜ì–´ëŠ”ì§€ ê²€ì¦í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ê²€ì¦ì€ ë³´ìˆ˜ì ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•˜ë©°, ì„±ê³µì´ 100% í™•ì‹¤í•˜ì§€ ì•Šì€ ëª¨ë“  ê²½ìš°ì—ëŠ” êµì°¨ ê²€ì¦ì„ í†µí•´ ì¬í™•ì¸ í•´ì•¼ í•©ë‹ˆë‹¤.

[ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´]
1. analyze_log(step, expected_result)
- ë¡œê·¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 'success' | 'fail' | 'uncertain'ì„ íŒë‹¨í•©ë‹ˆë‹¤.
- ì•ˆë“œë¡œì´ë“œ ì‹œìŠ¤í…œ ë‚´ë¶€ ë™ì‘ì„ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

2. adb_screen_vlm(question)
- í˜„ì¬ í™”ë©´ì˜ ìŠ¤í¬ë¦°ìƒ·ì„ ë¶„ì„í•˜ì—¬ expected_resultê°€ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- ì‹¤ì œ ì‚¬ìš©ì ê´€ì ì˜ UI ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ë° ê²°ì •ì ì…ë‹ˆë‹¤.

[í–‰ë™ ì§€ì¹¨]
1. ë¡œê·¸ ë¶„ì„
- ë¨¼ì € analyze_log íˆ´ì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

2. ì¡°ê±´ë¶€ êµì°¨ ê²€ì¦
- analyze_logì˜ ê²°ê³¼ê°€ 'success'ê°€ ì•„ë‹ ê²½ìš°, ë‹¹ì‹ ì€ ë°˜ë“œì‹œ adb_screen_vlm íˆ´ì„ ì¶”ê°€ë¡œ í˜¸ì¶œí•˜ì—¬ ì‹œê°ì  êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

3. ìµœì¢… ê²°ë¡  ì¢…í•©
- ë‘ ê°€ì§€ íˆ´ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤.
- adb_screen_vlmì˜ ê²°ê³¼ê°€ 'success'ë¼ë©´, ìµœì¢… ê²°ê³¼ë¥¼ 'success'ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë‘ íˆ´ ëª¨ë‘ì—ì„œ ì„±ê³µì„ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìµœì¢… ê²°ê³¼ë¥¼ 'fail'ë¡œ í™•ì •í•©ë‹ˆë‹¤.    

[ì¶”ê°€ ì •ë³´]
- Wi-Fi íŒì—…ì€ 'ë¡œë´‡ ì—°ê²°í•˜ê¸°' í™”ë©´ì…ë‹ˆë‹¤. Wi-Fi Popup í™”ë©´ì´ ì—´ë ¸ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²½ìš°ì—ë§Œ ì´ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì£¼ì„¸ìš”.

[ìµœì¢… ì¶œë ¥]
- ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
- "result" ê°’ì€ success | fail | uncertain ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
- "reason"ì€ ì–´ë–¤ ê·¼ê±°ë¡œ ìµœì¢… ê²°ë¡ ì„ ë‚´ë ¸ëŠ”ì§€ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
- "tools"ì—ëŠ” ë„ˆê°€ í˜¸ì¶œí•œ tool ì •ë³´ì™€ ì–´ë–¤ ì‘ë‹µì„ ë°›ì•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆœì„œëŒ€ë¡œ ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.

{{
    "result": "success" | "fail" | "uncertain",
    "reason": "ê°„ë‹¨í•œ ì´ìœ  ì„¤ëª…",
    "tools": "í˜¸ì¶œí•œ íˆ´ ìˆœì„œëŒ€ë¡œ ì‘ì„±"
}}
    """
    return [
        base.AssistantMessage(system_content),
        base.UserMessage(f"Current Step: {step}\nExpected Result: {expected_result}\n"),
    ]

@mcp.prompt()
def analysis_prompt(step: str, 
                            expected_result: str,
                            action_type: str,
                            canonical_name: str 
                            ) -> list[base.Message]:
    system_content = f"""
ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í˜„ì¬ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ ëª¨ë“  ê²€ì¦ íˆ´ì´ ì‹¤íŒ¨(fail)í–ˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ì„¸ìš”.

[í•„ìš” ì •ë³´]
- ìˆ˜í–‰ëœ step ë‚´ìš©: {step}
- step ìˆ˜í–‰ í›„ expected result: {expected_result}
- action_type: {action_type} (ì˜ˆ: tap, hold ë“±)
- step ì‹¤í–‰ì„ ìœ„í•´ í´ë¦­í•œ UI ì´ë¦„ : {canonical_name}

[ë¶„ì„ ëª©í‘œ]
- ì‹¤íŒ¨ì˜ ê°€ëŠ¥í•œ ì›ì¸ì„ ìƒì„¸íˆ ë¶„ì„
    - Action_mcpê°€ tool í˜¸ì¶œ ì¤‘ UIë¥¼ ëˆ„ë½í–ˆì„ ê°€ëŠ¥ì„±
    - í´ë¦­í•œ UIê°€ ì •ìƒì ìœ¼ë¡œ ëˆŒë¦¬ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±
    - Hold ë™ì‘ ì‹œê°„ ë¶€ì¡± ë“± action ê´€ë ¨ ë¬¸ì œ
    - ê¸°íƒ€ UI/í™˜ê²½ ë¬¸ì œ
    - UI ë§¤í•‘ì˜ ë¬¸ì œ (canonical name)
- í˜„ì¬ í™”ë©´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥ì„± ë° ë¬¸ì œ ì›ì¸ì„ ì¶”ì •

[ì¶œë ¥ í˜•ì‹]
- ë°˜ë“œì‹œ JSONìœ¼ë¡œ ì¶œë ¥
- ì˜ˆì‹œ:
{{
    "failure_reason": "tapì´ ì •ìƒ ìˆ˜í–‰ë˜ì§€ ì•ŠìŒ / Hold ì‹œê°„ ë¶€ì¡± ë“± ìƒì„¸ ì„¤ëª…",
    "recommendation": "í•´ê²°ì„ ìœ„í•´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ëŠ”ì§€ ê°„ë‹¨ ê¶Œì¥ì‚¬í•­"
}}
"""
    return [
        base.AssistantMessage(system_content),
        base.UserMessage(f"Current Step: {step}\nExpected Result: {expected_result}\nAction Type: {action_type}\nClicked UI Name: {canonical_name}"),
    ]

if __name__ == "__main__":
    mcp.run()
    #question = "The ì§€ì •ìœ„ì¹˜ ì´ë™ í™”ë©´ (PresetMove) opens, showing designated position options."
    #answer = query_screen_with_llm(Path("screen.png"), question)
    #print(answer)
